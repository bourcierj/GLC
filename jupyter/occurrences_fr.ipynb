{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import sys\n",
    "# add parent directory (root repo directory) to path\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set('talk')\n",
    "\n",
    "from IPython.display import display as idisplay\n",
    "\n",
    "# random seed\n",
    "RD_SEED = 0\n",
    "np.random.seed(RD_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis of occurrences for France"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is composed in multiple files:\n",
    "\n",
    "- `occurrences_fr_train.csv`\n",
    "- `occurrences_fr_test.csv`\n",
    "- `occurrences_us_train.csv`\n",
    "- `occurrences_us_test.csv`\n",
    "- `species_metadata.csv`\n",
    "\n",
    "The datasets columns include :\n",
    "\n",
    "- `id`:  The GLC20 reference identifier for the occurrence.\n",
    "- `lat`:\tDecimal latitude in the WGS84 coordinate system.\n",
    "- `lon`:\tDecimal longitude in the WGS84 coordinate system.\n",
    "- `species_id`:\tThe GLC20 reference identifier for the species.\n",
    "\n",
    "The metadata columns include :\n",
    "\n",
    "- `species_id`:\tThe GLC20 reference identifier for the species.\n",
    "- `GBIF_species_id`:\tThe GBIF reference identifier for the species.\n",
    "- `GBIF_species_name`:\tThe GBIF reference name for the species."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resume: statistical characteristics of the dataset\n",
    "\n",
    "### Dataset size and number of species\n",
    "\n",
    "The dataset is composed of 803k occurrences.  \n",
    "There are 8013 unique species in the dataset.\n",
    "\n",
    "\n",
    "### Missing values\n",
    "\n",
    "\n",
    "There are no missing values (non-attributes values) in the dataset. All fields are always present.\n",
    "\n",
    "### Duplicates\n",
    "\n",
    "#### 1. Duplicated rows (\"doublons\"):\n",
    "\n",
    "There are 165k duplicates in the dataset (i.e., records for which another record has the same (lat,lon,species_id) triplet). This equals to 20.5% of the dataset.\n",
    "\n",
    "\n",
    "Those are occurrences of the same specie identification, at exactly the same geolocation. Those samples probably come together, I mean that:\n",
    "- The person who identified the a specie with the PlantNet app, either identified the same specie by photographing a neighbouring plant or animal, at the same geolocation.\n",
    "- Or, she photographed to same plant or animal several time.\n",
    "\n",
    "Among these, not counting the first occurrence, there are 93k duplicates.This equals to 11.6% of the dataset. Those occurrences don't add any data, they just create an undesired effect of oversampling. So they will be removed.\n",
    "\n",
    "#### 2. Overlapping geolocations:\n",
    "\n",
    "Duplicates apart, there are 37k samples with overlapping geolocations. This equals to 4.7% of the dataset.\n",
    "\n",
    "Those are occurrences of different specie identification, at exactly the same location. Those overlapping samples probably come together, for the same reason as above: same person at the same spot.\n",
    "\n",
    "### Species\n",
    "\n",
    "#### 1. Species frequency as a function of the species frequency rank\n",
    "\n",
    "- Compared to GLC19, distribution is more evenly spread in the first hundred rank. This can be due to the per-species subsampling applied.  \n",
    "\n",
    "\n",
    "- The curve has an inverse shape visible when showing all species. This is characteristic of a Zipf law.  \n",
    "\n",
    "\n",
    "- Half of the dataset is covered by 400 species. 90% of the dataset is covered by 1500 species. The majority of species are under-represented.\n",
    "\n",
    "<!-- | Number of species (approx.) | Cumulated proportion |\n",
    "|-------------------|----------------------|\n",
    "| ~70               | 10%                  |\n",
    "| ~200              | 30%                  |\n",
    "| ~400              | 50%                  |\n",
    "| ~800              | 80%                  |\n",
    "| ~1500             | 90%                  | -->\n",
    "\n",
    "#### 2. Number of species filtered in as a function of the lower frequency threshold\n",
    "\n",
    "- The curve as expected shows an inverse relation. The point of inflexion of the curve is around a threshold of 50. The table below show a few possible threshold values with the number of species frequent enough. An acceptable value could be 5 or 10.\n",
    "\n",
    "<!-- | Frequency threshold | Number of species above |\n",
    "|-------------------|----------------------|\n",
    "| 2                 | 5312                 |\n",
    "| 3                 | 4807                 |\n",
    "| 4                 | 4456                 |\n",
    "| 5                 | 4151                 |\n",
    "| 10                | 3348                 |\n",
    "| 20                | 2640                 |\n",
    "| 50                | 1807                 |\n",
    "| 100               | 1201                 | -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the occurrences_fr_train.csv data\n",
    "PROJECT_ROOT = os.path.expanduser('~/projects/geolifeclef20/')\n",
    "DATA_ROOT = os.path.join(PROJECT_ROOT, 'data/')\n",
    "PATH_OCCURRENCES = os.path.join(DATA_ROOT, 'occurrences/')\n",
    "PATH_RASTERS = os.path.join(DATA_ROOT, 'rasters/')\n",
    "PATH_PATCHES = os.path.join(DATA_ROOT, 'patches/')\n",
    "PATH_FR_TRAIN = os.path.join(PATH_OCCURRENCES, 'occurrences_fr_train.csv')\n",
    "PATH_US_TRAIN = os.path.join(PATH_OCCURRENCES, 'occurrences_us_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(PATH_FR_TRAIN,\n",
    "    sep=';', header=0, index_col='id', low_memory=True)    \n",
    "df_metadata = pd.read_csv(os.path.join(PATH_OCCURRENCES,'species_metadata.csv'),\n",
    "    sep=';', header=0, index_col='species_id', low_memory=True)\n",
    "\n",
    "df = df.merge(\n",
    "    df_metadata, how='left', left_on='species_id', right_index=True,\n",
    "    copy=False).drop('GBIF_species_id', axis='columns')\n",
    "\n",
    "# target series is the species ids\n",
    "target_col = df['species_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Dataset sample:')\n",
    "idisplay(df.sample(20))\n",
    "dataset_size = len(df.index)\n",
    "num_species = len(target_col.unique())\n",
    "print('Dataset size: {}'.format(dataset_size))\n",
    "print('Number of species: {}'.format(num_species))\n",
    "\n",
    "print('Number of N/A rows: {}'.format(df.isna().any(axis=1).sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_noname = df.drop('GBIF_species_name', axis=1)\n",
    "dupl_mask = df_noname.duplicated(keep=False)\n",
    "dupl_mask_keepfirst = df_noname.duplicated(keep='first')\n",
    "\n",
    "df_nodupl = df_noname[dupl_mask_keepfirst]\n",
    "\n",
    "dupl_mask_geoloc = df_nodupl.duplicated(subset=['lat','lon'], keep=False)\n",
    "dupl_mask_geoloc_keepfirst = df_nodupl.duplicated(subset=['lat','lon'], keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_duplicates = dupl_mask.sum()\n",
    "n_duplicates_keepfirst = dupl_mask_keepfirst.sum()\n",
    "\n",
    "n_with_overlapping_geoloc = dupl_mask_geoloc.sum()\n",
    "n_with_overlapping_geoloc_keepfirst = dupl_mask_geoloc_keepfirst.sum()\n",
    "\n",
    "print('There are {} duplicates in the dataset (i.e., records for which another record has the same (lat,lon,species_id) triplet). '\n",
    "      'This equals to {:.1%} of the dataset.\\n'\\\n",
    "    .format(n_duplicates, n_duplicates / dataset_size))\n",
    "\n",
    "print('Among these, not counting the first occurrence, there are {} duplicates.'\n",
    "      'This equals to {:.1%} of the dataset.\\n'\\\n",
    "    .format(n_duplicates_keepfirst, n_duplicates_keepfirst / dataset_size))\n",
    "\n",
    "\n",
    "print('Duplicates apart, there are {} samples with overlapping geolocations.\\n'\\\n",
    "      'This equals to {:.1%} of the dataset.\\n'\\\n",
    "    .format(n_with_overlapping_geoloc,\n",
    "            n_with_overlapping_geoloc / dataset_size))\n",
    "\n",
    "print('Among these, not counting the first occurrence, there are {} overlapping geolocations.'\n",
    "      'This equals to {:.1%} of the dataset.\\n'\\\n",
    "    .format(n_with_overlapping_geoloc_keepfirst,\n",
    "            n_with_overlapping_geoloc_keepfirst / dataset_size))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Species frequency as a function of the species frequency rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "species, count_species = tuple(map(lambda x: pd.Series(x),\n",
    "                               np.unique(target_col, return_counts=True)))\n",
    "count_species.index = species\n",
    "# freqs = pd.DataFrame({'species': species, 'counts': count_species})\n",
    "# freqs = freqs.sort_values('counts', ascending=False)\n",
    "# freqs\n",
    "count_species = count_species.sort_values(ascending=False)\n",
    "count_species.name = 'count'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot of species frequency (count) as a function of the rank of the specie.\n",
    "# ax2 for cumulated proportion of occurrences\n",
    "# Help:\n",
    "# - https://matplotlib.org/gallery/api/two_scales.html\n",
    "# - https://matplotlib.org/3.1.0/gallery/subplots_axes_and_figures/secondary_axis.html\n",
    "\n",
    "max_rank = 1000\n",
    "counts = count_species.iloc[:max_rank]\n",
    "ranks = range(1, max_rank+1)\n",
    "# plot frequency\n",
    "fig, ax1 = plt.subplots(figsize=(12,8)) \n",
    "color1 = 'C0'\n",
    "ax1.plot(ranks, counts, color=color1)\n",
    "ax1.set_ylim(0)\n",
    "ax1.set_xlabel('Species rank')\n",
    "ax1.set_ylabel('Frequency', color=color1)\n",
    "ax1.tick_params(axis='y', labelcolor=color1)\n",
    "\n",
    "ax1.fill_between(ranks, counts, alpha=0.2)\n",
    "# add secondary axis for percentage of species ≤ rank\n",
    "secax1 = ax1.secondary_xaxis('top',\n",
    "                           functions=(lambda rank: rank / num_species,\n",
    "                                      lambda per: per * num_species)\n",
    "                          )\n",
    "secax1.set_xlabel('Percentage of species ≤ rank')\n",
    "\n",
    "cumulated_prop = np.cumsum(counts) / np.sum(count_species)\n",
    "# plot cumulated proportion of occurrences on different y axis\n",
    "ax2 = ax1.twinx()\n",
    "color2 = 'C1'\n",
    "ax2.plot(ranks, cumulated_prop, color=color2)\n",
    "ax2.set_ylabel('Cumulated proportion of dataset', color=color2)\n",
    "ax2.tick_params(axis='y', labelcolor=color2)\n",
    "# plt.savefig('figs/fr_frequency_rank{}.png'.format(max_rank), dpi=200, bbox_inches='tight')\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Number of species filtered in as a function of the lower frequency threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds = range(2, 200)\n",
    "# numer of species filtered in\n",
    "num_kept = pd.Series(((count_species >= t).sum() for t in thresholds), index=thresholds)\n",
    "# the number of occurrences it represent\n",
    "num_occs_kept = pd.Series((count_species[count_species >= t].sum() for t in thresholds), index=thresholds)\n",
    "idisplay(num_kept[[2,3,4,5,6,10,20]])\n",
    "idisplay(num_occs_kept[[2,3,4,5,6,10,20]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots(figsize=(12,8))\n",
    "p1, = ax1.plot(thresholds, num_kept)\n",
    "\n",
    "ax1.set_xlabel('Lower frequency threshold')\n",
    "ax1.set_ylabel('Number of species filtered in', color=p1.get_color())\n",
    "ax1.tick_params(axis='y', labelcolor=p1.get_color())\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "\n",
    "p2, = ax2.plot(thresholds, num_occs_kept / np.sum(count_species), color='C1')\n",
    "\n",
    "ax2.set_ylabel('Proportion of dataset', color=p2.get_color())\n",
    "ax2.tick_params(axis='y', labelcolor=p2.get_color())\n",
    "\n",
    "# secax1 = ax1.secondary_yaxis('right',\n",
    "#                            functions=(lambda num: num / num_species,\n",
    "#                                       lambda per: per * num_species)\n",
    "#                           )\n",
    "# secax1.set_ylabel('Percentage of species')\n",
    "plt.tight_layout()\n",
    "# plt.savefig('figs/fr_species_threshold.png', dpi=200, bbox_inches='tight')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
